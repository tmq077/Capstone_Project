{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83a224c",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project: \"DonateFoodGoWhere\" - Chatbot for donating specific food items in Singapore\n",
    "\n",
    "https://donatefoodgowhere.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd5566",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [About Retrieval Augmented Generation (RAG) and Fine-tuning](#About-Retrieval-Augmented-Generation-(RAG)-and-Fine-tuning)\n",
    "- [RAG](#RAG)\n",
    "- [Fine-tuning](#Fine-tuning)\n",
    "- [Evaluation](#Evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c944ea8a",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "In 2022, Singapore generated 813 million kg of food waste, accounting for 11% of total waste generated. Annually, each household throws away \\\\$258 worth of food, equivalent of 52 plates of nasi lemak (assuming each plate cost \\\\$5). The amount of food waste has grown by 20% over the past 10 years, and is expected to rise further. At current rate of waste disposal, Singapore will need a new incineration plant every 7-10 years, and a new landfill every 30-35 years. \n",
    "\n",
    "Households contribute around half of the food waste generated. As part of Singapore’s Zero Waste Masterplan, one key component of food waste management strategies is encouraging members of public to donate excess food. Organisations have specific wish list of food items and donation requirements, and it is time-consuming for individuals to find the right organisation for the food they wish to donate.\n",
    "\n",
    "This project aims to explore how we can help link individuals up with organisations, by developing a chatbot for individuals to enquire about donating specific food items, and find out where and how to donate, along with the relevant donation instructions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4e761",
   "metadata": {},
   "source": [
    "## About Retrieval Augmented Generation (RAG) and Fine-tuning\n",
    "\n",
    "In this project, I used the architecture that powers ChatGPT, a generative AI tool that has revolutionized the way users get answers to their questions. To build a custom chatbot using ChatGPT's Large Language Model (LLM), two techniques, Retriever Augmented Generation (RAG) and model Fine-tuning is used. \n",
    "\n",
    "RAG adjusts knowledge the LLM has access to through external knowledge retrieval, while fine-tuning adjusts the behaviour of the LLM for specific domains by training it on specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc96d46",
   "metadata": {},
   "source": [
    "## Import libraries, API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9888248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09accb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\" # replace with your API key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1853e04",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99211f6b",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "[LlamaIndex's documentation](https://gpt-index.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "LlamaIndex loads data via data connectors. The easiest and most commonly used data connector is the `SimpleDirectoryReader`, which creates documents out of every file in the given directory, 'webpages' in this case. 'webpages' consists of data of various charities saved from their respective websites in PDF and html format.\n",
    "<br>A `Document` is a collection of text data and metadata about that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af80e274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['783400e9-f868-4396-8136-04e0273ba75d', '466f67f2-0e62-4915-9fdd-cb3ac8ce4feb', '3dc45ba5-6383-4446-86b7-4c71ff2fd71a', '0e1064c9-1c57-4891-985f-70895abfd3bb', '5b18db31-9887-46c8-9930-a8052f679881', '3b8f3aa4-579a-456e-ae2f-3f2b6cd1394f', '8d3a28b0-a692-47b3-b1d3-b11b58d6d5be', '54a2533d-25be-4504-a9b6-0e4548c7d92b', '6cfdc969-b40c-446c-bd5e-419f1cfb68d7', '04ad98c5-2a79-491f-a496-71d1fd7624a0', '0cba5ae9-654a-408c-8c9f-eb8fdbbe4927', 'a4c9e7bf-586a-49b1-b637-3c85aa667dd0', '55e10171-5cf0-4757-b980-9173aac1603e', '736fa0fb-fbb6-44c3-86cb-0bbf9dc25daf', '4fc1d031-64f7-451e-92b7-b7c9748b38fa', '436e66c2-17ca-4f7a-b40d-45ae065469d0', 'd0d7589f-e02b-4d70-a0c6-974d746a0ebc', '7e9cb513-3bf3-4a62-86ed-77f5c319a803', 'b13336e5-29d2-49f9-acd4-e1b67ab7f25f', '7ea6ff6d-6cf7-4818-92b1-a8d961f88806', '273ad003-0702-4e84-a1df-a34aa39e6757', 'd4a611e1-038f-4175-98bd-6c14a22e63f1', '52db3589-80e6-4227-a74e-548a7db0b65b', 'ba01ddc3-89d3-4d2c-9af7-a6f81718e95f', 'ee9edb13-8af2-4602-8c01-d20cd07310e1', 'dd7d4834-e6bd-4e8f-88ab-d11b8cb795ec', 'b5e2a8ee-67e0-4f59-b1f0-ef26c73f0d7a', '19257789-2d46-4c0b-a467-64ded1a4e504', '7c84dcaa-42b9-439a-ae81-618464e91c7b', '1050bf52-5797-4efd-9572-81a5c5d3e455', 'ccce797c-883c-4d48-9f23-5b474ed2e64c', '6a96d164-8495-46f2-9767-bb23918018d6', '5809a7b7-299d-40b2-a527-29f5523fb929', '7732f0d3-d2d2-4552-ab7c-b6c35a60be8f', '88f78ef8-9548-43c7-923c-da086c31a52f', 'b0d18b64-cf24-4ba8-9517-38005829ccab', '1b2792dd-15e1-40a6-ac37-ba349c97b1d3', '0e7f8c97-bf6a-4fa7-97a6-6feaf64dc950', 'b2dfc0dc-f9ff-4957-af8d-eddae11e51dc', '9c203db3-a751-4a37-b673-586a5ca8ec17', 'df970cb3-9527-4841-a95c-28aaa80627f7', '2766dff7-a391-4b1a-97e2-9b552a20f015', '411d0ced-81fb-49ba-9e98-824bb833539f', '284027d8-78f8-4f13-9ac9-f3877162cbc4', '44aac6dd-266a-4bb1-8b0d-aa1e30f7e455', '033bfa26-e066-4a58-949a-5a22dfb14964', 'e837033c-0a8f-4d1a-9bd1-7af4ae9404ea', '4f45c25a-7063-403f-b114-90dbe2fae775', '0953efab-7f54-4d94-8f77-af96e618fa98', '5d75e713-daee-49bf-801c-747db8b37c6a', '10de6149-9a57-4136-a684-06f9fb0d6569', '6e025cff-1589-43e9-8ad0-fc1429176ccc', 'b1082904-6012-48ba-bcbc-22f1cf906ee7', '8c13eb5a-d4b4-46a9-8271-25398bc8921c', 'd31a7a22-acb8-413f-9bda-aae7a81671eb', 'd8b72642-591e-40fa-92c7-fee87b6866a5', '749ecdd8-34b1-485e-b800-26fbe34841cb', '54a56af9-3860-4330-aa72-5a4fb2fa437a', '447040f8-14ce-4495-989c-a2ecdef7af97', '2b0864e4-aa3a-4039-b4d8-ab9a7478b1e0', 'daf56971-1d4c-44c9-8b64-21f936fe0d6e', '51b4a6e6-299b-447a-a9f8-3c87e0241f14', '7b132a59-fbe5-448b-affa-82df031469ce', '14922895-bc5c-4ffa-9a00-5bc49f597f95', 'a294d791-a3f8-465b-9cd5-e68c72453411', 'b5eced8a-d883-40c9-8789-6382db6fde24', 'db1218a3-b6bb-4bd6-8eef-af02a917021c', '87fb7e00-3a50-4151-913a-83f291125200', 'f9b88a73-bf80-4162-930d-e94a28f518ea', 'c8a6a48c-8f32-438a-8a3b-ceae06d4a559']\n",
      "Loaded 70 docs\n"
     ]
    }
   ],
   "source": [
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "reader = SimpleDirectoryReader('webpages', exclude_hidden=True, file_metadata=filename_fn)\n",
    "docs = reader.load_data()\n",
    "print([x.doc_id for x in docs])\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba98350",
   "metadata": {},
   "source": [
    "### Indexing and Storage\n",
    "\n",
    "With all the data loaded, a list of 70 Document objects is created. Proceed to build an Index over these objects, which makes it ready for querying by an LLM. There are 4 types of indexing: Summary index, VectorStore Index, Tree Index and Keyword Table Index. \n",
    "\n",
    "In this project, `VectorStoreIndex` is used as it is by far the most frequent type of indexing. It takes the Documents and splits them up into Nodes, then creates `vector embeddings` of the text of every node. `Vector embedding` aka embedding is a numerical representation of the semantics, or meaning of the text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different. This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works.\n",
    "\n",
    "Definition of classes:\n",
    "- `ServiceContext` is a bundle of configuration data which can be passed to other stages of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5884ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LLM (gpt-3.5-turbo) from OpenAI and pass it to ServiceContext().\n",
    "# The GPTVectorStoreIndex will use gpt-3.5-turbo as embedding model to index the documents\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)) # degree of randomness from 0 to 1.  \n",
    "index = GPTVectorStoreIndex.from_documents(documents=docs, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dfbf8",
   "metadata": {},
   "source": [
    "After indexing, the ouput is stored in disk using the built-in .persist() method to avoid the time and cost of having to re-index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a86885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"data/index.vecstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7bea1",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Currently, there are three key integrations with [LlamaIndex for fine-tuning](https://gpt-index.readthedocs.io/en/latest/optimizing/fine-tuning/fine-tuning.html).\n",
    "\n",
    "In this project, since GPT-3.5-Turbo is used, I will try to distill a better model (e.g. GPT-4) into the simpler/cheaper model (e.g. GPT-3.5), i.e. finetuning GPT-3.5-turbo to ouput GPT-4 responses. \n",
    "\n",
    "The key steps are:\n",
    "1) Split the documents into train/evaluation set\n",
    "1) Generate a question/answer dataset over the train set\n",
    "    - use GPT-3.5-Turbo to generate questions from the external data, and GPT-4 query engine to generate answers.\n",
    "    - `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.\n",
    "2) Launch a finetuning job with `OpenAIFinetuneEngine`, and get back a finetuned model\n",
    "3) Evaluate the performance of the finetuned model and compare with the base model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f3a75",
   "metadata": {},
   "source": [
    "### Generate Train Dataset\n",
    "\n",
    "This dataset is for finetuning the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b839c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(docs)\n",
    "\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac047c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "# The below code is to unblock: nest the event loops\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009c3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are an individual food donor looking to donate specific food items. \\\n",
    "    Your task is to setup a quiz or examination. \\\n",
    "    Using the provided context from documents on different food support organisations, \\\n",
    "    formulate a single question that captures an important fact from the context. \\\n",
    "    Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "# find out more about question generation from \n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/evaluation/QuestionGeneration.html\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    docs[:40],\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56238a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "addee976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62eafadd",
   "metadata": {},
   "source": [
    "![train_questions.png](train_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366a4bd",
   "metadata": {},
   "source": [
    "### Generate Evaluation Dataset\n",
    "\n",
    "This dataset is for subsequent evaluation step to measure the performance of the models.\n",
    "<br> Questions are generated from a different set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0e58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    docs[\n",
    "        40:\n",
    "    ],  # since we generated question for the first 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50d7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  31  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62a9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da467b",
   "metadata": {},
   "source": [
    "![Screenshot of eval generated](eval_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66544e",
   "metadata": {},
   "source": [
    "### GPT-4 to Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86d9b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e69c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db75f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=gpt_4_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39ee8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1c6ac",
   "metadata": {},
   "source": [
    "### Create `OpenAIFinetuneEngine`\n",
    "\n",
    "`OpenAIFinetuneEngine` is a finetune engine that will take care of launching a finetuning job, and returning an LLM model that can be directly plugged in to the rest of LlamaIndex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "270f5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 43 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df8906c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"finetuning_events.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa06e60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 43\n",
      "First example:\n",
      "{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}\n",
      "{'role': 'user', 'content': \"Context information is below.\\n---------------------\\npage_label: 8\\nfile_name: webpages/foodfromtheheart.pdf\\n\\n30/10/2023, 14:19 In-Kind Food Donation in Singapore | Food from the Heart\\nhttps://www.foodfromtheheart.sg/in-kind-donations 4/5Evaporated/condensed milk\\nCooking oil, 500ml or 1L\\nBread spread\\n*Priority items\\n\\xa0\\nCommunity Shops Wish List ( )\\nCoﬀee in Sachets (All ﬂavours)\\nTea in Sachets (All ﬂavours)\\nMilo 3-in-1\\nAssorted canned pork products\\nAssorted Biscuits\\nCanned Fruits\\nCanned fried dace/ Sardines\\nRice 2.5kg - 5kg\\nCooking Oil 1L\\nThe below items are accepted but can only be donated during shop opening hours:\\nPacked fresh fruit\\nChilled Juice/ Milk\\nEggs\\n\\xa0\\n*Food from the Heart requires all food donations to be dated at least 3 months before expiry.\\nMake an Online Donation\\nWe also welcome cash contributions from anyone who wants to be a part of\\xa0our mission. Our\\ndedicated team will put your online donation towards safe-to-eat food for our beneﬁciaries and\\ncritical operating costs, helping our staﬀ and volunteers battle hunger in Singapore.\\xa0Make a\\ndonation today and make a diﬀerence in the lives of those less fortunate.\\nToy Donation Wish List\\n(Currently not collecting from the public, keep a lookout on our social media for updates!)\\nSports equipment\\nScooter\\nBoard games\\nDIY kits\\nConstruction blocks\\nCollected toys will be distributed to children aged 6-12 years old. New toys are preferred and\\npre-loved toys in good condition are welcome. We politely request to not receive pre-loved\\nstuﬀed toys and jigsaw puzzles.\\nWhere to Donate Toys in Singapore\\nDeliver your toy donations straight to the Food from the Heart warehouse during operating\\nhours.\\nCommunity Shop Locations Here\\n\\uf0c9\\n\\npage_label: 7\\nfile_name: webpages/foodfromtheheart.pdf\\n\\n30/10/2023, 14:19 In-Kind Food Donation in Singapore | Food from the Heart\\nhttps://www.foodfromtheheart.sg/in-kind-donations 3/5Community Shop @ Mountbatten\\nAddress: Blk 13 Old Airport Road #01-57 Singapore 390013\\nOpening Hours: Tuesday, Thursday, and Saturday from 9.30am to 12.30pm\\nCommunity Shop @ Boon Lay\\nAddress: Blk 176 Boon Lay Drive #01-366 Singapore 640176\\nOpening Hours: Tuesday, Thursday, Saturday, and Sunday from 9.30am to 12.30pm\\nCommunity Shop @ Lengkok Bahru\\nAddress: Blk 55 Lengkok Bahru #01-389 Singapore 151055\\nOpening Hours: Tuesday, Thursday, and Saturday from 9.30am to 12.30pm\\nCommunity Shop @ Punggol\\nAddress: Blk 224A Sumang Lane #01-02 Singapore 821224\\nOpening Hours: Tuesday, Thursday, and Saturday from 9.30am to 12.30pm\\nWe kindly request that donors refrain from dropping oﬀ perishable food items at the donation\\nboxes, as these items should only be donated in store during the shop's opening hours to\\nmaintain their freshness and quality.\\nMake an Online Food Delivery Donation\\nAt Food from the Heart, we understand that convenience is essential, and we welcome online\\nfood delivery donations from generous donors like you. Making an online food delivery\\ndonation is a simple yet impactful way to support our mission of alleviating hunger in\\nSingapore.\\nTo make an online food delivery donation, follow these easy steps:\\n\\x00. Choose the food items: Visit your preferred online grocery platforms, for\\nexample,\\xa0FairPrice\\xa0or\\xa0RedMart, and select the non-perishable food items listed in our Food\\nDonation Wish Lists you want to donate.\\n\\x00. Provide delivery instructions: During the checkout process, include the delivery address of\\nour oﬃce - 130 Joo Seng Road #03-01 Singapore 368357. If you plan to include perishable\\nitems in your donation, kindly instruct the delivery rider to deliver the items during our\\noﬃce hours (Monday to Friday, 9 am - 6 pm) to ensure the freshness and suitability of the\\ndonated items.\\nBy donating through online grocery platforms, you can conveniently contribute to our cause\\nwithout the need for physical drop-oﬀs.\\nThank you for your support!\\nFood Donation Wish Lists\\nCommunity Food Pack Wish List\\nRice, 1kg or 2kg\\nVermicelli/bee hoon\\nBiscuits, less sugar*\\nMalt drinks, less sugar, e.g. Milo*\\nCoﬀee or tea, less sugar*\\nCanned ﬁsh/meat*\\nCanned vegetables - especially beans, mushrooms, peas*\\nCanned mock meat\\nCanned soup\\nCanned fruit\\n\\uf0c9\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: What types of food items can be donated to Food from the Heart as an individual food donor?\\nAnswer: \"}\n",
      "{'role': 'assistant', 'content': 'Individuals can donate a variety of food items to Food from the Heart. These include evaporated or condensed milk, cooking oil (500ml or 1L), bread spread, coffee in sachets (all flavors), tea in sachets (all flavors), Milo 3-in-1, assorted canned pork products, assorted biscuits, canned fruits, canned fried dace or sardines, and rice (2.5kg - 5kg). They also accept packed fresh fruit, chilled juice or milk, and eggs, but these items can only be donated during shop opening hours. All food donations should be dated at least 3 months before expiry.'}\n",
      "No errors found\n",
      "Num examples missing system message: 3\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 2, 3\n",
      "mean / median: 2.9302325581395348, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 512, 1632\n",
      "mean / median: 979.7209302325581, 905.0\n",
      "p5 / p95: 623.2, 1426.2\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 12, 326\n",
      "mean / median: 68.69767441860465, 45.0\n",
      "p5 / p95: 19.0, 143.20000000000005\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~42128 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~126384 tokens\n",
      "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
      "This means your total cost for training will be $0.337024 per epoch.\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb0384",
   "metadata": {},
   "source": [
    "![notification of successful finetuned job](finetune_job.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e6b1581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-TELtB14BEZObAnOi4GnuilS9 at 0x7ff3e8fc0bd0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-TELtB14BEZObAnOi4GnuilS9\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1698723349,\n",
       "  \"finished_at\": 1698724106,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::8Fa1rB0k\",\n",
       "  \"organization_id\": \"org-uKgXyGnBOIt0jbTHoR4VVtz2\",\n",
       "  \"result_files\": [\n",
       "    \"file-KYK95HQQBYuOJhMXJ1gOIe2r\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-MrYX9OR6MpZcHNDODBOOGXnd\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 126126,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b9a6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_llm = finetune_engine.get_finetuned_model(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155643",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324ad69",
   "metadata": {},
   "source": [
    "To measure the performance of the pipeline, whether it is able to generate relevant and accurate responses given the external data source and a set of queries, we use 2 evaluation metrics from [`ragas` evaluation library](https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics). Ragas uses LLMs under the hood to compute the evaluations.\n",
    "\n",
    "The performance of the base model, gpt-3.5-turbo, will be compared with the fine-tuned model.\n",
    "\n",
    "Computation of evaluation metrics require 3 components: \n",
    "1) `Question`: A list of questions that could be asked about my external data/documents, generated using .generate_questions_from_nodes in above fine-tuning step<br>\n",
    "2) `Context`: Retrieved contexts corresponding to each question. The context represents (chunks of) documents that are relevant to the question, i.e. the source from where the answer will be generated.<br>\n",
    "3) `Answer`: Answer generated corresponding to each question from baseline and fine-tuned model.\n",
    "\n",
    "The two metrics are as follow:\n",
    "\n",
    "- `answer_relevancy` - Measures how relevant the generated answer is to the question, where an answer is considered relevant when it <u>directly</u> and <u>appropriately</u> addresses the orginal question, i.e. answers that are complete and do not include unnecessary or duplicated information. The metric does not consider factuality. It is computed using `question` and `answer`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing relevant answers. To calculate this score, the LLM is prompted to generate an appropriate question for the generated answer multiple times, and the mean cosine similarity between these generated questions and the original question is measured. The underlying idea is that if the generated answer accurately addresses the initial question, the LLM should be able to generate questions from the answer that align with the original question, i.e. high mean cosine similarity, translating to high score.\n",
    "\n",
    "\n",
    "- `faithfulness` - Measures how factually accurate is the generated answer, i.e. if the response was hallucinated, or based on factuality (from the context). It is computed from `answer` and `context`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing contextually accurate information. To calculate this score, the LLM identifies statements within the generated answer and verifies if each statement is supported by the retrieved context. The process then counts the number of statements within the generated answer that can be logically inferred from the context, and dvide by the total number of statements in the answer. \n",
    "\n",
    "Additional note: Cosine similarity is a metric used to measure how similar two items are. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The output value ranges from 0–1 where 0 means no similarity, whereas 1 means that both the items are 100% similar.\n",
    "<br>Hallucinations refer to instances where the language model produces information or claims that are not accurate or supported by the input context.\n",
    "\n",
    "Resources:\n",
    "<br>https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4\n",
    "<br>https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\n",
    "<br>https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c1798",
   "metadata": {},
   "source": [
    "### Evaluation of base model: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849d3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c5f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=gpt_context)\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the index\n",
    "# We configure the retriever to return the top 2 most similar documents, which is also the default setting\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e119ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27a0ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|████████████████████████████████████████████| 3/3 [20:48<00:00, 416.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:56<01:52, 56.25s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|████████████████████████████████████████████| 3/3 [21:58<00:00, 439.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.9167, 'faithfulness': 0.8692, 'answer_relevancy': 0.9697}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [faithfulness, answer_relevancy])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894aaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt_35 = result.to_pandas()\n",
    "\n",
    "# Export cleaned dataframe as .csv\n",
    "df_gpt_35.to_csv(\"eval/df_gpt_35.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c804716",
   "metadata": {},
   "source": [
    "### Evaluation of fine-tuned model\n",
    "\n",
    "Run the fine-tuned model on the evaluation dataset again to measure any performance increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1094c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef9fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in ft_llm directly into ServiceContext\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=ft_llm,\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=ft_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376fa2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab41cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|████████████████████████████████████████████| 3/3 [25:10<00:00, 503.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|████████████████████████████████████████████| 3/3 [11:56<00:00, 238.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.9255, 'faithfulness': 0.8779, 'answer_relevancy': 0.9784}\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [faithfulness, answer_relevancy])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e65fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_llm = result.to_pandas()\n",
    "\n",
    "# Export cleaned dataframe as .csv\n",
    "df_ft_llm.to_csv(\"eval/df_ft_llm.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415f4a75",
   "metadata": {},
   "source": [
    "| Pipeline                    | Answer Relevancy | Faithfulness |\n",
    "|-----------------------------|------------------|--------------|\n",
    "| RAG+GPT-3.5-Turbo           | 0.9697           | 0.8692       |\n",
    "| RAG+Finetuned GPT-3.5-Turbo | 0.9784           | 0.8779       |\n",
    "\n",
    "\n",
    "Both models are high in performance for both metrics, with the fine-tuned model having an improvement of 1%. \n",
    "Instead of fine-tuning, we could get comparable performance using the base model, which involves far less computing resource. \n",
    "And on top of that, as food items for organisation’s wishlist change over time, RAG alone can easily and quickly adapt to the new data. As such, RAG+GPT-3.5-Turbo is chosen for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b789ce",
   "metadata": {},
   "source": [
    "## Exploring Differences\n",
    "\n",
    "Let's quickly compare the differences in responses, to demonstrate that fine tuning did indeed change something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a4e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ce5b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00a403a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the contact information for donating dry rations to YWCA Singapore?\n"
     ]
    }
   ],
   "source": [
    "print(questions[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782afdb",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f92a9de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To donate dry rations to YWCA Singapore, you can contact Irene at 6223 1227 or email csp@ywca.org.sg. They will provide you with further information on how to fulfill the wishlist items and make your generous contributions.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=gpt_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbfcb5",
   "metadata": {},
   "source": [
    "### Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "579b2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=ft_llm,\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eee391a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contact person for donating dry rations to YWCA Singapore is Irene, and she can be reached at 6223 1227 or via email at csp@ywca.org.sg.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=ft_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b54718",
   "metadata": {},
   "source": [
    "The original base model generated additional sentence \"They will provide you with further information on how to fulfill the wishlist items and make your generous contributions.\" in the answer. This is considered unnecessary information as it is not directly related to the query. The generated answer by the fine-tuned model is concise and sufficiently informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62622a8",
   "metadata": {},
   "source": [
    "[Click for app code in streamlit](../streamlit/foodbot.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
